{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines (RBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be compred to a neural network with an input layer and a hidden layer.\n",
    "\n",
    "![RBM](rbm.png)\n",
    "\n",
    "Visible - v\n",
    "\n",
    "Hidden - h\n",
    "\n",
    "Can calculate h from v, and can calculate v from h.\n",
    "\n",
    "Bernoulli RBMs are practical for many web based applications where user actions are binary.\n",
    "\n",
    "## Key Calculations\n",
    "\n",
    "Computing from h to v:\n",
    "\n",
    "$$p(h = 1 | v) = \\sigma(W^T v + c)$$\n",
    "\n",
    "Computing from v to h:\n",
    "\n",
    "$$p(v = 1 | h) = \\sigma(Wh + b)$$\n",
    "\n",
    "Note - W is a shared weight.\n",
    "\n",
    "The RBM has many similarities to the Autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy of a Boltzmann Machine\n",
    "\n",
    "The goal of training a Boltzmann Machine is to find a thermal equilibrium.\n",
    "\n",
    "Boltzmann Machines are very difficult to train, however. This leads us to the Restricted Boltzmann Machine.\n",
    "\n",
    "Unlike the traditional Boltzmann Machine, the RBM does not allow interconnections within a layer, e.g. visible neurons can't connect to visible neurons.\n",
    "\n",
    "$$\n",
    "E(v, h) = - \\left(v^T W h + b^T v + c^T h \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Model\n",
    "\n",
    "This theory is derived from statistical mechanics. $p_i$ is the probability that a system is in a microstate with energy E_i.\n",
    "\n",
    "$$\n",
    "p(v, h) \\propto e^{-E(v,h)} = \\frac{1}{Z}e^{-E(v,h)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z = \\sum_v \\sum_h e^{-E(v,h)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Equations\n",
    "\n",
    "Our probability model can take us back to the neural network equations that we are familiar with today. All that is required is Bayes Theorem and some refactoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an RBM\n",
    "\n",
    "Slightly more complicated, can't just use SGD.\n",
    "\n",
    "### Free Energy\n",
    "\n",
    "We need to find something that is tractible to optimize.\n",
    "\n",
    "$$F(v) = -log \\sum_h e^{-E(v,h)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM's for Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix, csr_matrix, save_npz, load_npz\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def dot1(V, W):\n",
    "    # V is N x D x K (batch of visible units)\n",
    "    # W is D x K x M (weights)\n",
    "    # returns N x M (hidden layer size)\n",
    "    return tf.tensordot(V, W, axes=[[1,2], [0,1]])\n",
    "\n",
    "def dot2(H, W):\n",
    "    # H is N x M (batch of hiddens)\n",
    "    # W is D x K x M (weights transposed)\n",
    "    # returns N x D x K (visible)\n",
    "    return tf.tensordot(H, W, axes=[[1], [2]])\n",
    "\n",
    "\n",
    "class RBM(object):\n",
    "    def __init__(self, D, M, K):\n",
    "        self.D = D # input feature size\n",
    "        self.M = M # hidden size\n",
    "        self.K = K # number of ratings\n",
    "        self.build(D, M, K)\n",
    "\n",
    "\n",
    "    def build(self, D, M, K):\n",
    "        # params\n",
    "        self.W = tf.Variable(tf.random_normal(shape=(D, K, M)) * np.sqrt(2.0 / M))\n",
    "        self.c = tf.Variable(np.zeros(M).astype(np.float32))\n",
    "        self.b = tf.Variable(np.zeros((D, K)).astype(np.float32))\n",
    "\n",
    "        # data\n",
    "        self.X_in = tf.placeholder(tf.float32, shape=(None, D))\n",
    "\n",
    "        # one hot encode X\n",
    "        # first, make each rating an int\n",
    "        X = tf.cast(self.X_in * 2 - 1, tf.int32)\n",
    "        X = tf.one_hot(X, K)\n",
    "\n",
    "        # conditional probabilities\n",
    "        # NOTE: tf.contrib.distributions.Bernoulli API has changed in Tensorflow v1.2\n",
    "        V = X\n",
    "        p_h_given_v = tf.nn.sigmoid(dot1(V, self.W) + self.c)\n",
    "        self.p_h_given_v = p_h_given_v # save for later\n",
    "\n",
    "        # draw a sample from p(h | v)\n",
    "        r = tf.random_uniform(shape=tf.shape(p_h_given_v))\n",
    "        H = tf.to_float(r < p_h_given_v)\n",
    "\n",
    "        # draw a sample from p(v | h)\n",
    "        # note: we don't have to actually do the softmax\n",
    "        logits = dot2(H, self.W) + self.b\n",
    "        cdist = tf.distributions.Categorical(logits=logits)\n",
    "        X_sample = cdist.sample() # shape is (N, D)\n",
    "        X_sample = tf.one_hot(X_sample, depth=K) # turn it into (N, D, K)\n",
    "\n",
    "        # mask X_sample to remove missing ratings\n",
    "        mask2d = tf.cast(self.X_in > 0, tf.float32)\n",
    "        mask3d = tf.stack([mask2d]*K, axis=-1) # repeat K times in last dimension\n",
    "        X_sample = X_sample * mask3d\n",
    "\n",
    "\n",
    "        # build the objective\n",
    "        objective = tf.reduce_mean(self.free_energy(X)) - tf.reduce_mean(self.free_energy(X_sample))\n",
    "        self.train_op = tf.train.AdamOptimizer(1e-2).minimize(objective)\n",
    "        # self.train_op = tf.train.GradientDescentOptimizer(1e-3).minimize(objective)\n",
    "\n",
    "        # build the cost\n",
    "        # we won't use this to optimize the model parameters\n",
    "        # just to observe what happens during training\n",
    "        logits = self.forward_logits(X)\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=X,\n",
    "                logits=logits,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # to get the output\n",
    "        self.output_visible = self.forward_output(X)\n",
    "\n",
    "\n",
    "        # for calculating SSE\n",
    "        self.one_to_ten = tf.constant((np.arange(10) + 1).astype(np.float32) / 2)\n",
    "        self.pred = tf.tensordot(self.output_visible, self.one_to_ten, axes=[[2], [0]])\n",
    "        mask = tf.cast(self.X_in > 0, tf.float32)\n",
    "        se = mask * (self.X_in - self.pred) * (self.X_in - self.pred)\n",
    "        self.sse = tf.reduce_sum(se)\n",
    "\n",
    "        # test SSE\n",
    "        self.X_test = tf.placeholder(tf.float32, shape=(None, D))\n",
    "        mask = tf.cast(self.X_test > 0, tf.float32)\n",
    "        tse = mask * (self.X_test - self.pred) * (self.X_test - self.pred)\n",
    "        self.tsse = tf.reduce_sum(tse)\n",
    "\n",
    "\n",
    "        initop = tf.global_variables_initializer()\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(initop)\n",
    "\n",
    "    def fit(self, X, X_test, epochs=10, batch_sz=256, show_fig=True):\n",
    "        N, D = X.shape\n",
    "        n_batches = N // batch_sz\n",
    "\n",
    "\n",
    "        costs = []\n",
    "        test_costs = []\n",
    "        for i in range(epochs):\n",
    "            t0 = datetime.now()\n",
    "            print(\"epoch:\", i)\n",
    "            X, X_test = shuffle(X, X_test) # everything has to be shuffled accordingly\n",
    "            for j in range(n_batches):\n",
    "                x = X[j*batch_sz:(j*batch_sz + batch_sz)].toarray()\n",
    "\n",
    "                _, c = self.session.run(\n",
    "                    (self.train_op, self.cost),\n",
    "                    feed_dict={self.X_in: x}\n",
    "                )\n",
    "\n",
    "                if j % 100 == 0:\n",
    "                    print(\"j / n_batches:\", j, \"/\", n_batches, \"cost:\", c)\n",
    "            print(\"duration:\", datetime.now() - t0)\n",
    "\n",
    "            # calculate the true train and test cost\n",
    "            t0 = datetime.now()\n",
    "            sse = 0\n",
    "            test_sse = 0\n",
    "            n = 0\n",
    "            test_n = 0\n",
    "            for j in range(n_batches):\n",
    "                x = X[j*batch_sz:(j*batch_sz + batch_sz)].toarray()\n",
    "                xt = X_test[j*batch_sz:(j*batch_sz + batch_sz)].toarray()\n",
    "\n",
    "                # number of train ratings\n",
    "                n += np.count_nonzero(x)\n",
    "\n",
    "                # number of test ratings\n",
    "                test_n += np.count_nonzero(xt)\n",
    "\n",
    "                # use tensorflow to get SSEs\n",
    "                sse_j, tsse_j = self.get_sse(x, xt)\n",
    "                sse += sse_j\n",
    "                test_sse += tsse_j\n",
    "            c = sse/n\n",
    "            ct = test_sse/test_n\n",
    "            print(\"train mse:\", c)\n",
    "            print(\"test mse:\", ct)\n",
    "            print(\"calculate cost duration:\", datetime.now() - t0)\n",
    "            costs.append(c)\n",
    "            test_costs.append(ct)\n",
    "        if show_fig:\n",
    "            plt.plot(costs, label='train mse')\n",
    "            plt.plot(test_costs, label='test mse')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def free_energy(self, V):\n",
    "        first_term = -tf.reduce_sum(dot1(V, self.b))\n",
    "        second_term = -tf.reduce_sum(\n",
    "            # tf.log(1 + tf.exp(tf.matmul(V, self.W) + self.c)),\n",
    "            tf.nn.softplus(dot1(V, self.W) + self.c),\n",
    "            axis=1\n",
    "        )\n",
    "        return first_term + second_term\n",
    "\n",
    "    def forward_hidden(self, X):\n",
    "        return tf.nn.sigmoid(dot1(X, self.W) + self.c)\n",
    "\n",
    "    def forward_logits(self, X):\n",
    "        Z = self.forward_hidden(X)\n",
    "        return dot2(Z, self.W) + self.b\n",
    "\n",
    "    def forward_output(self, X):\n",
    "        return tf.nn.softmax(self.forward_logits(X))\n",
    "\n",
    "    def transform(self, X):\n",
    "        # accepts and returns a real numpy array\n",
    "        # unlike forward_hidden and forward_output\n",
    "        # which deal with tensorflow variables\n",
    "        return self.session.run(self.p_h_given_v, feed_dict={self.X_in: X})\n",
    "\n",
    "    def get_visible(self, X):\n",
    "        return self.session.run(self.output_visible, feed_dict={self.X_in: X})\n",
    "\n",
    "    def get_sse(self, X, Xt):\n",
    "        return self.session.run(\n",
    "            (self.sse, self.tsse),\n",
    "            feed_dict={\n",
    "            self.X_in: X,\n",
    "            self.X_test: Xt,\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    A = load_npz(\"A-train.npz\")\n",
    "    A_test = load_npz(\"A-test.npz\")\n",
    "\n",
    "    N, M = A.shape\n",
    "    rbm = RBM(M, 50, 10)\n",
    "    rbm.fit(A, A_test)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
