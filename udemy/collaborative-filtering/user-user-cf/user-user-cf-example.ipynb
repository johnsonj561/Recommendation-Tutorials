{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from sortedcontainers import SortedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/movielens/rating.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User IDS are sequential from 1-138493. We will re-inded them to be 0 based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.userId = df.userId - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie IDS are 1 - 131262, but not all IDS are used. We will create a new 0 based index for movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping for movie ids\n",
    "unique_movie_ids = set(df.movieId.values)\n",
    "movie2idx = {}\n",
    "count = 0\n",
    "for movie_id in unique_movie_ids:\n",
    "  movie2idx[movie_id] = count\n",
    "  count += 1\n",
    "\n",
    "    \n",
    "# add them to the data frame\n",
    "df['movie_idx'] = df.apply(lambda row: movie2idx[row.movieId], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing timestamp, we don't need it\n",
    "df = df.drop(columns=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of edited ratings\n",
    "df.to_csv('../../data/movielens/edited_rating.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Sample\n",
    "\n",
    "We're going to work with the top 5K users and 2K movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000263, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/movielens/edited_rating.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of users\n",
    "N = df.userId.max() + 1 \n",
    "# number of movies\n",
    "M = df.movie_idx.max() + 1\n",
    "\n",
    "user_ids_count = Counter(df.userId)\n",
    "movie_ids_count = Counter(df.movie_idx)\n",
    "\n",
    "# number of users and movies to keep\n",
    "n = 5000\n",
    "m = 2000\n",
    "\n",
    "user_ids = [u for u, c in user_ids_count.most_common(n)]\n",
    "movie_ids = [m for m, c in movie_ids_count.most_common(m)]\n",
    "\n",
    "keep_mask = df.userId.isin(user_ids) & df.movie_idx.isin(movie_ids)\n",
    "df_small = df.loc[keep_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 5000\n",
      "j: 2000\n",
      "max user id: 4999\n",
      "max movie id: 1999\n",
      "small dataframe size: 3399948\n"
     ]
    }
   ],
   "source": [
    "# need to remake user ids and movie ids since they are no longer sequential\n",
    "new_user_id_map = {}\n",
    "i = 0\n",
    "for old in user_ids:\n",
    "  new_user_id_map[old] = i\n",
    "  i += 1\n",
    "print(\"i:\", i)\n",
    "\n",
    "new_movie_id_map = {}\n",
    "j = 0\n",
    "for old in movie_ids:\n",
    "  new_movie_id_map[old] = j\n",
    "  j += 1\n",
    "print(\"j:\", j)\n",
    "\n",
    "df_small.loc[:, 'userId'] = df_small \\\n",
    "    .apply(lambda row: new_user_id_map[row.userId], axis=1)\n",
    "df_small.loc[:, 'movie_idx'] = df_small \\\n",
    "    .apply(lambda row: new_movie_id_map[row.movie_idx], axis=1)\n",
    "\n",
    "print(\"max user id:\", df_small.userId.max())\n",
    "print(\"max movie id:\", df_small.movie_idx.max())\n",
    "print(\"small dataframe size:\", len(df_small))\n",
    "\n",
    "df_small.to_csv('../../data/movielens/small_rating.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/movielens/small_rating.csv')\n",
    "\n",
    "N = df.userId.max() + 1 # number of users\n",
    "M = df.movie_idx.max() + 1 # number of movies\n",
    "\n",
    "df = shuffle(df)\n",
    "cutoff = int(0.8*len(df))\n",
    "\n",
    "df_train = df.iloc[:cutoff]\n",
    "df_test = df.iloc[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map users -> list of movies ated\n",
    "user2movie = {}\n",
    "\n",
    "# map movies -> list of users who have rated\n",
    "movie2user = {}\n",
    "\n",
    "# map user-movie pairs -> ratings\n",
    "usermovie2rating = {}\n",
    "\n",
    "count = 0\n",
    "def update_user2movie_and_movie2user(row):\n",
    "    global count\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(\"processed: %.3f\" % (float(count)/cutoff))\n",
    "\n",
    "    i = int(row.userId)\n",
    "    j = int(row.movie_idx)\n",
    "    if i not in user2movie:\n",
    "        user2movie[i] = [j]\n",
    "    else:\n",
    "        user2movie[i].append(j)\n",
    "\n",
    "    if j not in movie2user:\n",
    "        movie2user[j] = [i]\n",
    "    else:\n",
    "        movie2user[j].append(i)\n",
    "    usermovie2rating[(i,j)] = row.rating\n",
    "    \n",
    "    \n",
    "df_train.apply(update_user2movie_and_movie2user, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test ratings dictionary\n",
    "usermovie2rating_test = {}\n",
    "\n",
    "count = 0\n",
    "def update_usermovie2rating_test(row):\n",
    "    global count\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(\"processed: %.3f\" % (float(count)/len(df_test)))\n",
    "    i = int(row.userId)\n",
    "    j = int(row.movie_idx)\n",
    "    usermovie2rating_test[(i,j)] = row.rating\n",
    "\n",
    "df_test.apply(update_usermovie2rating_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('user2movie.json', 'wb') as f:\n",
    "    pickle.dump(user2movie, f)\n",
    "\n",
    "with open('movie2user.json', 'wb') as f:\n",
    "    pickle.dump(movie2user, f)\n",
    "\n",
    "with open('usermovie2rating.json', 'wb') as f:\n",
    "    pickle.dump(usermovie2rating, f)\n",
    "\n",
    "with open('usermovie2rating_test.json', 'wb') as f:\n",
    "    pickle.dump(usermovie2rating_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing User Similarity Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 5000 M: 2000\n"
     ]
    }
   ],
   "source": [
    "with open('user2movie.json', 'rb') as f:\n",
    "    user2movie = pickle.load(f)\n",
    "\n",
    "with open('movie2user.json', 'rb') as f:\n",
    "    movie2user = pickle.load(f)\n",
    "\n",
    "with open('usermovie2rating.json', 'rb') as f:\n",
    "    usermovie2rating = pickle.load(f)\n",
    "\n",
    "with open('usermovie2rating_test.json', 'rb') as f:\n",
    "    usermovie2rating_test = pickle.load(f)\n",
    "\n",
    "\n",
    "N = np.max(list(user2movie.keys())) + 1\n",
    "\n",
    "# the test set may contain movies the train set doesn't have data on\n",
    "m1 = np.max(list(movie2user.keys()))\n",
    "m2 = np.max([m for (u, m), r in usermovie2rating_test.items()])\n",
    "M = max(m1, m2) + 1\n",
    "print(\"N:\", N, \"M:\", M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the user similarities, you have to do O(N^2 * M) calculations!\n",
    "# in the \"real-world\" you'd want to parallelize this\n",
    "# note: we really only have to do half the calculations, since w_ij is symmetric\n",
    "\n",
    "# number of neighbors we'd like to consider\n",
    "K = 25 \n",
    "\n",
    "# number of common movies users must have in common in order to consider\n",
    "limit = 5 \n",
    "\n",
    "# store neighbors in this list\n",
    "neighbors = [] \n",
    "\n",
    "# each user's average rating for later use\n",
    "averages = [] \n",
    "\n",
    "# each user's deviation for later use\n",
    "deviations = []\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    # find the 25 closest users to user i\n",
    "    movies_i = user2movie[i]\n",
    "    movies_i_set = set(movies_i)\n",
    "\n",
    "    # calculate avg and deviation\n",
    "    ratings_i = { movie:usermovie2rating[(i, movie)] for movie in movies_i }\n",
    "    avg_i = np.mean(list(ratings_i.values()))\n",
    "    dev_i = { movie:(rating - avg_i) for movie, rating in ratings_i.items() }\n",
    "    dev_i_values = np.array(list(dev_i.values()))\n",
    "    sigma_i = np.sqrt(dev_i_values.dot(dev_i_values))\n",
    "\n",
    "    # save these for later use\n",
    "    averages.append(avg_i)\n",
    "    deviations.append(dev_i)\n",
    "\n",
    "    sl = SortedList()\n",
    "    for j in range(N):\n",
    "        if i == j:\n",
    "            continue\n",
    "        movies_j = user2movie[j]\n",
    "        movies_j_set = set(movies_j)\n",
    "        common_movies = (movies_i_set & movies_j_set) # intersection\n",
    "        if len(common_movies) > limit:\n",
    "            # calculate avg and deviation\n",
    "            ratings_j = { movie:usermovie2rating[(j, movie)] for movie in movies_j }\n",
    "            avg_j = np.mean(list(ratings_j.values()))\n",
    "            dev_j = { movie:(rating - avg_j) for movie, rating in ratings_j.items() }\n",
    "            dev_j_values = np.array(list(dev_j.values()))\n",
    "            sigma_j = np.sqrt(dev_j_values.dot(dev_j_values))\n",
    "\n",
    "            # calculate correlation coefficient\n",
    "            numerator = sum(dev_i[m]*dev_j[m] for m in common_movies)\n",
    "            w_ij = numerator / (sigma_i * sigma_j)\n",
    "\n",
    "            # insert into sorted list and truncate\n",
    "            # negate weight, because list is sorted ascending\n",
    "            # maximum value (1) is \"closest\"\n",
    "            sl.add((-w_ij, j))\n",
    "            if len(sl) > K:\n",
    "              del sl[-1]\n",
    "\n",
    "    # store the neighbors\n",
    "    neighbors.append(sl)\n",
    "\n",
    "    # print out useful things\n",
    "    if i % 1 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(i, m):\n",
    "    # calculate the weighted sum of deviations\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for neg_w, j in neighbors[i]:\n",
    "        # remember, the weight is stored as its negative\n",
    "        # so the negative of the negative weight is the positive weight\n",
    "        try:\n",
    "            numerator += -neg_w * deviations[j][m]\n",
    "            denominator += abs(neg_w)\n",
    "        except KeyError:\n",
    "      # neighbor may not have rated the same movie\n",
    "      # don't want to do dictionary lookup twice\n",
    "      # so just throw exception\n",
    "          pass\n",
    "\n",
    "    if denominator == 0:\n",
    "        prediction = averages[i]\n",
    "    else:\n",
    "        prediction = numerator / denominator + averages[i]\n",
    "    prediction = min(5, prediction)\n",
    "    prediction = max(0.5, prediction) # min rating is 0.5\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = []\n",
    "train_targets = []\n",
    "for (i, m), target in usermovie2rating.items():\n",
    "    # calculate the prediction for this movie\n",
    "    prediction = predict(i, m)\n",
    "\n",
    "    # save the prediction and target\n",
    "    train_predictions.append(prediction)\n",
    "    train_targets.append(target)\n",
    "\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "# same thing for test set\n",
    "for (i, m), target in usermovie2rating_test.items():\n",
    "    # calculate the prediction for this movie\n",
    "    prediction = predict(i, m)\n",
    "\n",
    "    # save the prediction and target\n",
    "    test_predictions.append(prediction)\n",
    "    test_targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 0.5698880768762259\n",
      "test mse: 0.6293990316519572\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "def mse(p, t):\n",
    "    p = np.array(p)\n",
    "    t = np.array(t)\n",
    "    return np.mean((p - t)**2)\n",
    "\n",
    "print('train mse:', mse(train_predictions, train_targets))\n",
    "print('test mse:', mse(test_predictions, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
