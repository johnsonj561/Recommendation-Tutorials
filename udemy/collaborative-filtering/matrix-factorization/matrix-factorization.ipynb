{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "Matrix factorization is a model-based collaborative filtering method that decomposes the rating matrix R into two factors, i.e. a user feature matrix and a item feature matrix.\n",
    "\n",
    "$$\\text{Ratings Matrix R} = (N \\times M)$$\n",
    "\n",
    "$$N = \\text{number of users}$$\n",
    "\n",
    "$$M = \\text{number of items}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that R is often *very large* and either can't fit into memory or will not scale as the total number of users increases.\n",
    "\n",
    "**Problem:** We need a more efficient way to represent item-user matrix R.\n",
    "\n",
    "**Goal:** Express matrix R using two smaller matrices *W* and *U*.\n",
    "\n",
    "$$\\hat{R} = WU^T$$\n",
    "\n",
    "Where $\\hat{R}$ is an approximation of R. We want it to be as close to R as possible, i.e. minimize error or loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factors\n",
    "\n",
    "We want *W* and *U* to be relatively small to reduce computational complexity.\n",
    "\n",
    "$$U^{N \\times K} = \\text{users matrix}$$\n",
    "\n",
    "$$W^{M \\times K} = \\text{items matrix}$$\n",
    "\n",
    "Typically use $K \\in [10,50]$, but this is a hyperparameter that should be varied in order to maximize performance.\n",
    "\n",
    "Note - there are many factors to ratings matrix R. We select a value of K and then optimize W and U in order to reduce total error, where $error = f(R, \\hat{R})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Reduction in Complexity\n",
    "\n",
    "Let K = 10, N = 130k, M = 26k\n",
    "\n",
    "Ratings matrix $R = N \\times M = 3.38$ billion.\n",
    "\n",
    "User matrix $U = N \\times 10 = 1.3$ million.\n",
    "\n",
    "Item matrix $W = M \\times 10 = 260$k.\n",
    "\n",
    "$U + W = 1.56 \\text{million}$\n",
    "\n",
    "**As we can see, working with matrix factors reduces computational complexity by several orders of magnitude.**\n",
    "\n",
    "$$\\frac{1.56 \\text{million}}{3.38 \\text{billion}} = 0.05\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Ratings from U and W\n",
    "\n",
    "Avoid multiplying *U* by *W* directly, the resulting $\\hat{R}$ will be very large and consume all resources.\n",
    "\n",
    "Instead, compute individual scalars at a time and write results to memory.\n",
    "\n",
    "$$\\hat{r}_{ij} = \\hat{R}[i,j] = w_i^T u_i$$\n",
    "$$w_i = W[i], \\text{and } u_j = U[j]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Does This Work\n",
    "\n",
    "Singular value decomposition (SVD) tells us that a matrix *X* can be decomposed into 3 separate matrices multiplied together.\n",
    "\n",
    "$$X = USV^T$$\n",
    "\n",
    "If we can decompose a matrix into 3 matrices, then of course we can also decompose a matrix into 2 matrices. Just multiply $U \\times S$.\n",
    "\n",
    "![SVD breakdown](svd-breakdown.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Matrix Factors\n",
    "\n",
    "We can treat the K columns of *U* and *W* as features. These features are learned during model optimization.\n",
    "\n",
    "When we take the dot product $w_i^T u_j$, we get something similar to the cosine similarity. This tells us how well a user's attributes correlate with the item's attributes.\n",
    "\n",
    "$$\n",
    "w_i^T u_j = ||w_i|| \\text{ } ||u_j|| cos\\theta \\propto sim(i,j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we have K = 2 and we are *pretending* that the K features represent *comedy* and *action*.\n",
    "\n",
    "We predict the ratings of $\\hat{R}$ by taking the dot product.\n",
    "\n",
    "![Matrix Factors Visual](matrix-factors-ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we factor R into U and W, U and W become user and item feature matrices, respectively.\n",
    "\n",
    "**We don't actually know the real meaning of our learned K features.**\n",
    "\n",
    "These features are latent features, and K is the latent dimensionality. These are also known as \"hidden causes\"\n",
    "\n",
    "If we want to get insight on what the features represent, we can cluster users on a single feature and observe which users or items form groups.\n",
    "\n",
    "Matrix Factorization has automatically found the optimal features for us by learning to predict $\\hat{R}$ s.t. error is minimized. \n",
    "\n",
    "We don't have to perform manual feature engineering, and performance is usually better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
