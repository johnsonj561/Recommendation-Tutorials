{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "Matrix factorization is a model-based collaborative filtering method that decomposes the rating matrix R into two factors, i.e. a user feature matrix and a item feature matrix.\n",
    "\n",
    "$$\\text{Ratings Matrix R} = (N \\times M)$$\n",
    "\n",
    "$$N = \\text{number of users}$$\n",
    "\n",
    "$$M = \\text{number of items}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that R is often *very large* and either can't fit into memory or will not scale as the total number of users increases.\n",
    "\n",
    "**Problem:** We need a more efficient way to represent item-user matrix R.\n",
    "\n",
    "**Goal:** Express matrix R using two smaller matrices *W* and *U*.\n",
    "\n",
    "$$\\hat{R} = WU^T$$\n",
    "\n",
    "Where $\\hat{R}$ is an approximation of R. We want it to be as close to R as possible, i.e. minimize error or loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factors\n",
    "\n",
    "We want *W* and *U* to be relatively small to reduce computational complexity.\n",
    "\n",
    "$$U^{N \\times K} = \\text{users matrix}$$\n",
    "\n",
    "$$W^{M \\times K} = \\text{items matrix}$$\n",
    "\n",
    "Typically use $K \\in [10,50]$, but this is a hyperparameter that should be varied in order to maximize performance.\n",
    "\n",
    "Note - there are many factors to ratings matrix R. We select a value of K and then optimize W and U in order to reduce total error, where $error = f(R, \\hat{R})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Reduction in Complexity\n",
    "\n",
    "Let K = 10, N = 130k, M = 26k\n",
    "\n",
    "Ratings matrix $R = N \\times M = 3.38$ billion.\n",
    "\n",
    "User matrix $U = N \\times 10 = 1.3$ million.\n",
    "\n",
    "Item matrix $W = M \\times 10 = 260$k.\n",
    "\n",
    "$U + W = 1.56 \\text{million}$\n",
    "\n",
    "**As we can see, working with matrix factors reduces computational complexity by several orders of magnitude.**\n",
    "\n",
    "$$\\frac{1.56 \\text{million}}{3.38 \\text{billion}} = 0.05\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Ratings from U and W\n",
    "\n",
    "Avoid multiplying *U* by *W* directly, the resulting $\\hat{R}$ will be very large and consume all resources.\n",
    "\n",
    "Instead, compute individual scalars at a time and write results to memory.\n",
    "\n",
    "$$\\hat{r}_{ij} = \\hat{R}[i,j] = w_i^T u_i$$\n",
    "$$w_i = W[i], \\text{and } u_j = U[j]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Does This Work\n",
    "\n",
    "Singular value decomposition (SVD) tells us that a matrix *X* can be decomposed into 3 separate matrices multiplied together.\n",
    "\n",
    "$$X = USV^T$$\n",
    "\n",
    "If we can decompose a matrix into 3 matrices, then of course we can also decompose a matrix into 2 matrices. Just multiply $U \\times S$.\n",
    "\n",
    "![SVD breakdown](svd-breakdown.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Matrix Factors\n",
    "\n",
    "We can treat the K columns of *U* and *W* as features. These features are learned during model optimization.\n",
    "\n",
    "When we take the dot product $w_i^T u_j$, we get something similar to the cosine similarity. This tells us how well a user's attributes correlate with the item's attributes.\n",
    "\n",
    "$$\n",
    "w_i^T u_j = ||w_i|| \\text{ } ||u_j|| cos\\theta \\propto sim(i,j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we have K = 2 and we are *pretending* that the K features represent *comedy* and *action*.\n",
    "\n",
    "We predict the ratings of $\\hat{R}$ by taking the dot product.\n",
    "\n",
    "![Matrix Factors Visual](matrix-factors-ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we factor R into U and W, U and W become user and item feature matrices, respectively.\n",
    "\n",
    "**We don't actually know the real meaning of our learned K features.**\n",
    "\n",
    "These features are latent features, and K is the latent dimensionality. These are also known as \"hidden causes\"\n",
    "\n",
    "If we want to get insight on what the features represent, we can cluster users on a single feature and observe which users or items form groups.\n",
    "\n",
    "Matrix Factorization has automatically found the optimal features for us by learning to predict $\\hat{R}$ s.t. error is minimized. \n",
    "\n",
    "We don't have to perform manual feature engineering, and performance is usually better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving For $w_i$\n",
    "\n",
    "$$R \\approx \\hat{R} = WU^T$$\n",
    "\n",
    "How do we approximate $\\hat{R}$ such that $R \\approx \\hat{R}$?\n",
    "\n",
    "This is a regression problem, so we want to minimize the Sum of Squared Errors.\n",
    "\n",
    "$$\n",
    "J = \\sum_{i,j \\in \\Omega}(r_{ij} - \\hat{r}_{ij})^2\n",
    "= \\sum_{i,j \\in \\Omega}(r_{ij} - w_i^T u_j)^2\n",
    "$$\n",
    "\n",
    "This is a convex function, so we can set the gradient to 0 and solve for the parameters.\n",
    "\n",
    "Recall:\n",
    "\n",
    "$$\\Omega = \\text{the set of pairs (i,j) where user i has rated movie j}$$\n",
    "\n",
    "$$\\Psi_i = \\text{the set of all movies j that user i has rated}$$\n",
    "\n",
    "Since we are taking the derivative with respect to $w_i$, we only care about the subset of ratings that depend on $w_i$. Therefore, the summation will only care about the subset of ratingns that depend on user *i* has rated. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} =\n",
    "2 \\sum_{j \\in \\Psi_i}(r_{ij} - w_i^T u_j)(-u_j) = 0\n",
    "$$\n",
    "\n",
    "Dot products are commutative, so we can change order. Therefore:\n",
    "\n",
    "$$\n",
    "\\sum_{j \\in \\Psi_i}r_{ij}u_j = \\sum_{j \\in \\Psi_i}(u_j^T w_i)u_j\n",
    " = \\sum_{j \\in \\Psi_i}u_j u_j^T w_i \n",
    "$$\n",
    "\n",
    "Since this is a summation over j and $w_i$ does not depend on j.\n",
    "\n",
    "$$\n",
    "\\left( \\sum_{j \\in \\Psi_i}u_j u_j^T \\right) w_i =\n",
    "\\sum_{j \\in \\Psi_i}r_{ij}u_j\n",
    "$$\n",
    "\n",
    "Now we are left with Ax = b, and we can solve for x.\n",
    "\n",
    "$$\n",
    "w_i = \\left( \\sum_{j \\in \\Psi_i}u_j u_j^T \\right)^{-1} \\sum_{j \\in \\Psi_i}r_{ij}u_j\n",
    "$$\n",
    "\n",
    "```python\n",
    "x = np.linalg.solve(A, b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving For $u_i$\n",
    "\n",
    "The loss is symmetric in W and U, so steps are identical.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial u_j} =\n",
    "2 \\sum_{i \\in \\Omega_j}(r_{ij} - w_i^T u_j)(-w_i) = 0\n",
    "$$\n",
    "\n",
    "Set the derivative to 0, isolate terms, and put into form Ax = b:\n",
    "\n",
    "$$\n",
    "u_j = \\left( \\sum_{i \\in \\Omega_j}w_i w_i^T \\right)^{-1}\n",
    "\\sum_{i \\in \\Omega_j}r_{ij}w_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Way Dependency Between W and U\n",
    "\n",
    "Solving for W depennds on U, and solving for U depends on W.\n",
    "\n",
    "### Alternating Least Squares\n",
    "\n",
    "Initialize W and U to random numbers.\n",
    "\n",
    "```python\n",
    "W = randn(N, K)\n",
    "U = randn(M, K)\n",
    "```\n",
    "\n",
    "Then apply the two equations iteratively, solving for W and then U.\n",
    "\n",
    "$$\n",
    "w_i = \\left( \\sum_{j \\in \\Psi_i}u_j u_j^T \\right)^{-1} \\sum_{j \\in \\Psi_i}r_{ij}u_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_j = \\left( \\sum_{i \\in \\Omega_j}w_i w_i^T \\right)^{-1}\n",
    "\\sum_{i \\in \\Omega_j}r_{ij}w_i\n",
    "$$\n",
    "\n",
    "Each iteration will take 1 step towards the global minimum.\n",
    "\n",
    "It does not matter which matrix we solve for first, because W and U are symmetric.\n",
    "\n",
    "Let's say you update W first, and then U. You do not need to use *old* values of W when solving for U. It is more efficient to use the new values for *W*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating User Bias\n",
    "\n",
    "Recall our intro to User-Based Collaborative Filtering, we used rating deviations instead of raw rating. We achieved this by substracting a user's average rating from each item rating. We did this because each user has a bias and may be optimistic/pessimistic with respect to ratings.\n",
    "\n",
    "We should incorporate a similar bias into our Matrix Factorization.\n",
    "\n",
    "$$\n",
    "\\hat{r}_{ij} = w_i^T u_j + b_i + c_j + u\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_i = \\text{ user bias}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_j = \\text{ item bias}\n",
    "$$\n",
    "\n",
    "$$\n",
    "u = \\text{ global average}\n",
    "$$\n",
    "\n",
    "The global average $u$ centers the data set, i.e. shifting ratings so that the mean is 0.\n",
    "\n",
    "User bias is discussed in User-User CF - users may be optimistic or pessimistic in their ratings.\n",
    "\n",
    "Item bias is important because two movies can have the same features, e.g. action, comedy, etc., but one can be terribly rated compared to the other.\n",
    "\n",
    "### Training with Bias Terms\n",
    "\n",
    "$$J = \\sum_{i,j \\in \\Omega}(r_{ij} - \\hat{r}_{ij})^2$$\n",
    "\n",
    "$$\\hat{r}_{ij} = w_i^T u_j + b_i + c_j + u$$\n",
    "\n",
    "#### Differentiat wrt $w_i$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} =\n",
    "2 \\sum_{j \\in \\Psi_i}(r_{ij} - w_i^T u_j - b_i - c_j - u)(-u_j) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{j \\in \\Psi_i}(w_i^T u_j)u_j = \n",
    "\\sum_{j \\in \\Psi_i}(r_{ij} - b_i - c_j - u)u_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_i = \\left(\\sum_{j \\in \\Psi_i}u_j u_j^T\\right)^{-1}\n",
    "\\sum_{j \\in \\Psi_i}(r_{ij} - b_i - c_j - u)u_j\n",
    "$$\n",
    "\n",
    "#### Differentiating wrt $u_j$\n",
    "\n",
    "$$\n",
    "u_j = \\left(\\sum_{i \\in \\Omega_j}w_i w_i^T\\right)^{-1}\n",
    "\\sum_{i \\in \\Omega_j}(r_{ij} - b_i - c_j - u)w_i\n",
    "$$\n",
    "\n",
    "#### Differentiating wrt $b_i$\n",
    "\n",
    "Since $b_i$ appears by itself its derivative is 1 and all other terms go to 0.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_i} =\n",
    "2 \\sum_{j \\in \\Psi_i}(r_{ij} - w_i^T u_j - b_i - c_j - u)(-1) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_i = \\frac{1}{\\mid \\Psi_i \\mid} \\sum_{j \\in \\Psi_i}(r_{ij} - w_i^T u_j - c_j - u)\n",
    "$$\n",
    "\n",
    "The Bias for user i is the average deviation of the actual rating minus every other part of the model.\n",
    "\n",
    "#### Differentiating wrt $c_j$\n",
    "\n",
    "Similar to $b_i$.\n",
    "\n",
    "$$\n",
    "c_j = \\frac{1}{\\mid \\Psi_j \\mid} \\sum_{i \\in \\Psi_j}(r_{ij} - w_i^T u_j - b_i - u)\n",
    "$$\n",
    "\n",
    "#### Computing $u$\n",
    "\n",
    "$u$ is a global average and does not need to be updated. Just take it from the training data prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
